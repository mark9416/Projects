---
title: "Housing"
author: "Yu Chieh Cheng $ HeJin Chu"
date: "2023-04-28"
output: word_document
---

Our dataset is from Kaggle website, called Housing Prices Dataset. https://www.kaggle.com/datasets/yasserh/housing-prices-dataset

```{r}
library(tidyverse)
library(readr)
library(dplyr)
library(broom)
Housing <- read_csv("Housing.csv")
Housing
```

1. Offer a preliminary description of the data set. For example, indicate the size of the data source, describe the variables, and include any other data profile information that would be of interest.
-> there are 545 rows and 13 columns include in this data set.
-> the dependent variable we want to look at is house price, and the independent variables we want to discuss is area,bedrooms,bathrooms,stories,hot water heating,airconditioning,parking and furnishing status

```{r}
dim(Housing)
```
```{r}
Housing%>%
  select(price,area,bedrooms,bathrooms,stories,hotwaterheating,airconditioning,parking,furnishingstatus)->
Housing1
Housing%>%
  select(price,area,bedrooms,bathrooms,stories,parking)->Housing2
```

2. Generate relevant data visual plots that explore multicollinearity for the quantitative variables and normality for the quantitative variables as well. Also, use R code to confirm the levels of the categorical variables.

```{r}
library(corrplot)
corrplot(cor(Housing2), method = "number")
```

```{r}
hist(Housing2$area, col = "lightblue", border = "lavender")
hist(Housing2$bedrooms, col = "lightblue", border = "lavender")
hist(Housing2$bathrooms, col = "lightblue", border = "lavender")
hist(Housing2$stories, col = "lightblue", border = "lavender")
hist(Housing2$parking, col = "lightblue", border = "lavender")
```
```{r}
unique(Housing1$hotwaterheating)
```
```{r}
unique(Housing1$airconditioning)
```
```{r}
unique(Housing1$furnishingstatus)
```
3. Using R code, produce a full Regression Model that consists of quantitative and categorical variables.  Make use of the R generated dummy variable matrices

```{r}
hu <- model.matrix(~hotwaterheating-1, data=Housing1)
hwhyes <- hu[,"hotwaterheatingyes"]
hwhno <- hu[,"hotwaterheatingno"]

ac <- model.matrix(~airconditioning-1, data=Housing1)
acyes <- ac[,"airconditioningyes"]
acno <- ac[,"airconditioningno"]

fs <- model.matrix(~furnishingstatus-1, data=Housing1)
fsfurnished <- fs[,"furnishingstatusfurnished"]
fssemifurnished <- fs[,"furnishingstatussemi-furnished"]
fsunfurnished <- fs[,"furnishingstatusunfurnished"]
```

```{r}
Housing1%>%
  mutate(hotwaterheatingyes=hwhyes)%>%
  mutate(hotwaterheatingno=hwhno)%>%
  mutate(airconditioningyes=acyes)%>%
  mutate(airconditioninno=acno)%>%
  mutate(furnished=fsfurnished)%>%
  mutate(semifurnished=fssemifurnished)%>%
  mutate(unfurnished=fsunfurnished)%>%
  select(-hotwaterheating,-airconditioning,-furnishingstatus)-> Housing3
Housing3
```
```{r}
Housingmodel<-lm(price~.,data = Housing1)
Housingmodel
```
4. Using only the quantitative variables as predictors, produce a model using matrix methods. Also use matrix methods to find the fitted values and the residuals

```{r}
Ym<-matrix(Housing$price,ncol = 1,byrow = TRUE)
```

```{r}
Xm<-as.matrix(Housing2)
Xm[Xm>20000]<-1
```

```{r}
t(Xm) -> transposeX
transposeX
transposeX%*%Xm -> Product
solve(Product)%*%transposeX%*%Ym->interpretandslopes
interpretandslopes
```
```{r}
Xm%*%interpretandslopes->fittedvalue
fittedvalue
Ym-fittedvalue
```


5. Produce an output summary table to be used to analyze and evaluate the full model (Adjusted R squared, Standard Error, Significance of Variables, ectâ€¦)

There are 63%, bedrooms and furnishingstatussemi-furnished are not significant.

```{r}
summary(Housingmodel)
```
6. Use procedures and techniques explored in class to produce confidence intervals for the independent quantitative variables of your model. Choose at least two of the quantitative variables to find confidence intervals for.

```{r}
Housingmodel2<-lm(price~.,data=Housing2)
tidy(Housingmodel2, conf.int = TRUE)
```

7. Now produce a reduced model (removing variables of your choice with justification). Use R summary coding for both models and offer justification for choosing one model over the other.

```{r}
Housing4<-Housing3%>%
  select(-bedrooms,-semifurnished,-hotwaterheatingno,-airconditioninno)
reducedmodel<-lm(price~.,data = Housing4)
summary(reducedmodel)
```
8. Research and apply a model analysis technique not discussed in class to your full model or reduced model.  Fully explain the technique or procedure and how it is being applied to your specific model.

Features of Random Forest

Aggregates many decision trees: A random forest is a collection of decision trees and thus, does not rely on a single feature and combines multiple predictions from each decision tree.

Prevents overfitting: With multiple decision trees, each tree draws a sample random data giving the random forest more randomness to produce much better accuracy than decision trees.

Advantages of Random Forest

Efficient: Random forests are much more efficient than decision trees while performing on large databases.

Highly accurate: Random forests are highly accurate as they are collection of decision trees and each decision tree draws sample random data and in result, random forests produces higher accuracy on prediction.

Efficient estimates of the test error: It makes efficient use of all predictive features and maintains accuracy even if the data is missing.

```{r}
library(randomForest)
library(ggplot2)
set.seed(4543)
rf.fit <- randomForest(price ~ ., data=Housing4)
rf.fit
which.min(rf.fit$mse)
sqrt(rf.fit$mse[which.min(rf.fit$mse)]) 
```

```{r}
plot(rf.fit)
varImpPlot(rf.fit)
```

9)	Offer final summary perspectives about the data and the models that you produce, suggesting how your models or model analysis enhanced your understanding of the data.

In the beginning, we build full regression model and then remove the insignificant variables based on the result. After making the result of reduced model, we find multiple R-squared is 0.6274, which means the price could be explained of 62.74% by this model. Furthermore, we can know the top three factors of housing price are area, bathrooms, and air conditioning. Something interesting about this data is we expect bedroom would be one of the important factors but it's not. The result shows bedroom is not significant.